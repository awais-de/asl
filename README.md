# ğŸ–ï¸ English-to-Sign Language (ASL) Translation System
*A Semester-Long NLP + Computer Vision Project*

## ğŸ“œ Overview
This project implements a **complex, multi-stage pipeline** that translates English sentences into **American Sign Language (ASL)**, then renders the translation as **sign videos** or **3D avatar animations**.

Unlike simple rule-based or one-step implementations, this system integrates:
- **Natural Language Processing (NLP)** for grammar-aware English â†’ ASL gloss translation.
- **Computer Vision (CV)** for sign video retrieval and pose-based rendering.
- **Dataset fusion** from multiple large-scale ASL datasets.

## ğŸ¯ Project Goals
1. **Translate** natural English text into **ASL gloss** (a written representation of sign language).
2. **Align gloss tokens** with real-world ASL sign videos or pose sequences.
3. **Render** an animated or video-based output for end-users.
4. **Enable scalability** for more sign languages and larger vocabularies.

## ğŸ“‚ Directory Structure
```
asl_project/
â”œâ”€â”€ data/                  # Datasets and preprocessing outputs
â”‚   â”œâ”€â”€ raw/               # Original datasets (ASLG-PC12, WLASL, How2Sign)
â”‚   â”œâ”€â”€ processed/         # Tokenized and cleaned datasets
â”‚   â”œâ”€â”€ poses/             # Extracted pose keypoints from sign videos
â”œâ”€â”€ src/                   # Source code
â”‚   â”œâ”€â”€ config.py          # Paths, constants, and hyperparameters
â”‚   â”œâ”€â”€ preprocess.py      # Dataset cleaning, tokenization, alignment
â”‚   â”œâ”€â”€ translation.py     # English â†’ ASL gloss transformer model
â”‚   â”œâ”€â”€ retrieval.py       # Sign video retrieval / pose matching
â”‚   â”œâ”€â”€ render.py          # Video player or avatar rendering
â”‚   â”œâ”€â”€ evaluate.py        # Model evaluation metrics
â”‚   â”œâ”€â”€ app.py             # CLI or web app interface
â”œâ”€â”€ notebooks/             # Jupyter notebooks for experiments
â”œâ”€â”€ models/                # Saved ML models and checkpoints
â”œâ”€â”€ requirements.txt       # Python dependencies
â”œâ”€â”€ README.md              # This file
â””â”€â”€ LICENSE
```

## ğŸ”„ Pipeline Overview
1. **Data Preparation**: Download & preprocess ASLG-PC12, How2Sign, WLASL datasets.
2. **NLP Model**: Fine-tune transformer (T5/mBART) for English â†’ ASL gloss.
3. **Gloss Mapping**: Map gloss tokens to sign videos or generate fingerspelling.
4. **Pose Rendering**: Render 3D avatar or 2D pose keypoints.
5. **Output**: Play sign video sequence or avatar animation.

## ğŸ“Š Datasets
- ASLG-PC12
- WLASL
- How2Sign
- SignBank

## ğŸ’» Installation
```
git clone https://github.com/yourusername/asl_project.git
cd asl_project
python3 -m venv venv
source venv/bin/activate
pip install -r requirements.txt
```

## ğŸš€ Usage
```
python src/app.py --mode video
```

## ğŸ“ˆ Evaluation Metrics
- BLEU / ROUGE for translation
- Retrieval accuracy
- User comprehension studies

## ğŸ“… Timeline
- Weeks 1-2: Literature review & dataset collection
- Weeks 3-4: Preprocessing
- Weeks 5-6: Model training
- Weeks 7-8: Retrieval
- Weeks 9-10: Rendering
- Weeks 11-12: Evaluation
- Weeks 13-14: Report & presentation

## ğŸ“œ License
MIT License
